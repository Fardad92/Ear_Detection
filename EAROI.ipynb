{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EAROI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fardad92/Ear_Detection/blob/main/EAROI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRPIv51redae"
      },
      "source": [
        "!git clone https://github.com/Fardad92/Ear_Detection.git\n",
        "import sys\n",
        "sys.path.append('Ear_Detection/')\n",
        "from models.resnet import rf_lw152\n",
        "from utils.helpers import prepare_img\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "cmap = np.load('Ear_Detection/utils/cmap.npy')\n",
        "\n",
        "\n",
        "model = rf_lw152(2, imagenet=False)\n",
        "device = next(model.parameters()).device\n",
        "print(device)\n",
        "model.eval()\n",
        "\n",
        "cap = cv2.VideoCapture('input1.mp4')  #you can change this line for using imported video file\n",
        "length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
        "\n",
        "if not cap.isOpened:\n",
        "    print('--(!)Error opening video capture')\n",
        "    exit(0)\n",
        "else:\n",
        "    ret, frame1 = cap.read()\n",
        "\n",
        "out = cv2.VideoWriter('output.avi', fourcc, 5.0, (270, 360))\n",
        "i=0\n",
        "while cap.isOpened():\n",
        "    ret, frame1 = cap.read()\n",
        "    if frame1 is None:\n",
        "        print('--(!) No captured frame -- Break!')\n",
        "        break\n",
        "    if cv2.waitKey(10) == 27:\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        cv2.destroyAllWindows()\n",
        "        break    \n",
        "\n",
        "\n",
        "    frames = cv2.resize(frame1, (360, 270))\n",
        "    framet = cv2.rotate(frames, cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      img = cv2.cvtColor(framet, cv2.COLOR_BGR2RGB)\n",
        "      orig_size = img.shape[:2][::-1]\n",
        "      img_inp = torch.from_numpy(prepare_img(img).transpose(2, 0, 1)[None]).float()\n",
        "\n",
        "      segm = model(img_inp)\n",
        "      segm = segm[0].data.cpu().numpy().argmax(axis=0).astype(np.uint8)\n",
        "      segm = cv2.resize(segm, (orig_size), interpolation=cv2.INTER_NEAREST)\n",
        "      segm = cmap[segm]\n",
        "\n",
        "      gray = cv2.cvtColor(segm, cv2.COLOR_BGR2GRAY)\n",
        "      edged = cv2.Canny(gray, 10, 20)\n",
        "      contours, hierarchy = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "      seg_con = cv2.drawContours(img, contours, -1, (0, 255, 0), 3)\n",
        "      print(100*(i/length))\n",
        "      i += 1\n",
        "      seg_con = cv2.cvtColor(seg_con, cv2.COLOR_RGB2BGR)\n",
        "      clear_output(wait=True)\n",
        "      cv2_imshow(seg_con)\n",
        "      out.write(seg_con)\n",
        "\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}